{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bPVTxb4akHi"
      },
      "source": [
        "**Vamos experimentar agora a Rede Neural Artificial?**\n",
        "Veja:\n",
        "https://scikit-learn.org/stable/modules/neural_networks_supervised.html# "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "fpe0EYaXiIPm"
      },
      "outputs": [],
      "source": [
        "!pip -q install yellowbrick\n",
        "!pip -q install imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "ru9xg6QIaceV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer, accuracy_score, confusion_matrix, classification_report\n",
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from scipy import stats\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Ler a base CSV**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "STeZ46Y4bKfl"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('breast-cancer.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Convertendo os dados nominais para dados numéricos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\eduol\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "categorical_cols = ['age', 'menopause', 'tumor-size', 'inv-nodes', 'node-caps', 'breast', 'breast-quad', 'irradiat']\n",
        "\n",
        "encoder = OneHotEncoder(sparse=False, drop='first')\n",
        "encoded_data = encoder.fit_transform(data[categorical_cols])\n",
        "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(input_features=categorical_cols))\n",
        "\n",
        "data = pd.concat([data.drop(columns=categorical_cols), encoded_df], axis=1)\n",
        "data['Class'] = data['Class'].map({'no-recurrence-events': 0, 'recurrence-events': 1})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Eliminar redundâncias nos dados através de colunas altamente correlacionadas**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                       deg-malig     Class  age_30-39  age_40-49  age_50-59  \\\n",
            "deg-malig               1.000000  0.299400   0.046313   0.016290   0.023117   \n",
            "Class                   0.299400  1.000000   0.099192   0.004147   0.057214   \n",
            "age_30-39               0.046313  0.099192   1.000000   0.257143   0.269737   \n",
            "age_40-49               0.016290  0.004147   0.257143   1.000000   0.481673   \n",
            "age_50-59               0.023117  0.057214   0.269737   0.481673   1.000000   \n",
            "age_60-69               0.045018  0.001138   0.189322   0.338075   0.354632   \n",
            "age_70-79               0.109045  0.041811   0.055549   0.099195   0.104053   \n",
            "menopause_lt40          0.071934  0.003982   0.008110   0.107335   0.079082   \n",
            "menopause_premeno       0.031758  0.052386   0.340224   0.509545   0.257235   \n",
            "tumor-size_10-14        0.213471  0.188487   0.054072   0.020552   0.009931   \n",
            "tumor-size_15-19        0.069193  0.047840   0.042098   0.109112   0.001690   \n",
            "tumor-size_20-24        0.006901  0.022960   0.008151   0.104384   0.054257   \n",
            "tumor-size_25-29        0.040695  0.038140   0.021472   0.019373   0.054377   \n",
            "tumor-size_30-34        0.117266  0.134684   0.014302   0.020690   0.002543   \n",
            "tumor-size_35-39        0.134697  0.041569   0.025751   0.030867   0.018504   \n",
            "tumor-size_40-44        0.034242  0.015460   0.048686   0.054338   0.017100   \n",
            "tumor-size_45-49        0.006839  0.008140   0.039070   0.004134   0.073186   \n",
            "tumor-size_5-9          0.088719  0.077449   0.044567   0.016589   0.021605   \n",
            "tumor-size_50-54        0.011268  0.028877   0.064373   0.023629   0.014131   \n",
            "inv-nodes_12-14         0.132876  0.083233   0.039070   0.151941   0.073186   \n",
            "inv-nodes_15-17         0.155812  0.064956   0.055549   0.058415   0.050943   \n",
            "inv-nodes_24-26         0.076446  0.091089   0.022478   0.040139   0.042105   \n",
            "inv-nodes_3-5           0.117832  0.145320   0.046667   0.030159   0.001873   \n",
            "inv-nodes_6-8           0.183968  0.160084   0.082926   0.074820   0.009197   \n",
            "inv-nodes_9-11          0.090675  0.126112   0.042536   0.006019   0.025933   \n",
            "node-caps_no            0.271713  0.274964   0.001415   0.020592   0.062491   \n",
            "node-caps_yes           0.325930  0.276792   0.025263   0.007165   0.041103   \n",
            "breast_right            0.052861  0.058646   0.039441   0.103086   0.073879   \n",
            "breast-quad_central     0.073271  0.065738   0.095237   0.104159   0.026998   \n",
            "breast-quad_left_low    0.013504  0.036290   0.003333   0.025000   0.016390   \n",
            "breast-quad_left_up     0.052634  0.045711   0.049202   0.007563   0.008750   \n",
            "breast-quad_right_low   0.037217  0.031259   0.037220   0.093621   0.054905   \n",
            "breast-quad_right_up    0.020563  0.076444   0.038069   0.038069   0.001783   \n",
            "irradiat_yes            0.208099  0.193912   0.035669   0.081381   0.136102   \n",
            "\n",
            "                       age_60-69  age_70-79  menopause_lt40  \\\n",
            "deg-malig               0.045018   0.109045        0.071934   \n",
            "Class                   0.001138   0.041811        0.003982   \n",
            "age_30-39               0.189322   0.055549        0.008110   \n",
            "age_40-49               0.338075   0.099195        0.107335   \n",
            "age_50-59               0.354632   0.104053        0.079082   \n",
            "age_60-69               1.000000   0.073033        0.034264   \n",
            "age_70-79               0.073033   1.000000        0.023187   \n",
            "menopause_lt40          0.034264   0.023187        1.000000   \n",
            "menopause_premeno       0.523957   0.153735        0.166350   \n",
            "tumor-size_10-14        0.071263   0.033872        0.023961   \n",
            "tumor-size_15-19        0.086293   0.029510        0.093471   \n",
            "tumor-size_20-24        0.045284   0.003144        0.046245   \n",
            "tumor-size_25-29        0.039413   0.070624        0.076419   \n",
            "tumor-size_30-34        0.022399   0.075425        0.029537   \n",
            "tumor-size_35-39        0.097942   0.039050        0.042254   \n",
            "tumor-size_40-44        0.045481   0.140859        0.045725   \n",
            "tumor-size_45-49        0.120458   0.015072        0.016309   \n",
            "tumor-size_5-9          0.015115   0.017434        0.018865   \n",
            "tumor-size_50-54        0.074611   0.024832        0.026870   \n",
            "inv-nodes_12-14         0.051367   0.015072        0.016309   \n",
            "inv-nodes_15-17         0.073033   0.021429        0.023187   \n",
            "inv-nodes_24-26         0.118729   0.008671        0.009383   \n",
            "inv-nodes_3-5           0.021774   0.055549        0.060107   \n",
            "inv-nodes_6-8           0.022651   0.036800        0.039819   \n",
            "inv-nodes_9-11          0.094965   0.104954        0.030150   \n",
            "node-caps_no            0.036862   0.020059        0.023540   \n",
            "node-caps_yes           0.047662   0.072232        0.078159   \n",
            "breast_right            0.005152   0.039661        0.058030   \n",
            "breast-quad_central     0.006219   0.052328        0.044590   \n",
            "breast-quad_left_low    0.037366   0.065579        0.014311   \n",
            "breast-quad_left_up     0.049323   0.001802        0.029915   \n",
            "breast-quad_right_low   0.087862   0.043690        0.047940   \n",
            "breast-quad_right_up    0.043202   0.023497        0.013621   \n",
            "irradiat_yes            0.050322   0.024447        0.088465   \n",
            "\n",
            "                       menopause_premeno  tumor-size_10-14  ...  \\\n",
            "deg-malig                       0.031758          0.213471  ...   \n",
            "Class                           0.052386          0.188487  ...   \n",
            "age_30-39                       0.340224          0.054072  ...   \n",
            "age_40-49                       0.509545          0.020552  ...   \n",
            "age_50-59                       0.257235          0.009931  ...   \n",
            "age_60-69                       0.523957          0.071263  ...   \n",
            "age_70-79                       0.153735          0.033872  ...   \n",
            "menopause_lt40                  0.166350          0.023961  ...   \n",
            "menopause_premeno               1.000000          0.016146  ...   \n",
            "tumor-size_10-14                0.016146          1.000000  ...   \n",
            "tumor-size_15-19                0.062476          0.112774  ...   \n",
            "tumor-size_20-24                0.022559          0.151635  ...   \n",
            "tumor-size_25-29                0.119475          0.158936  ...   \n",
            "tumor-size_30-34                0.025252          0.169742  ...   \n",
            "tumor-size_35-39                0.085324          0.087880  ...   \n",
            "tumor-size_40-44                0.066697          0.095100  ...   \n",
            "tumor-size_45-49                0.039407          0.033918  ...   \n",
            "tumor-size_5-9                  0.005837          0.039235  ...   \n",
            "tumor-size_50-54                0.008314          0.055885  ...   \n",
            "inv-nodes_12-14                 0.029315          0.033918  ...   \n",
            "inv-nodes_15-17                 0.007174          0.048224  ...   \n",
            "inv-nodes_24-26                 0.062209          0.019514  ...   \n",
            "inv-nodes_3-5                   0.023616          0.089542  ...   \n",
            "inv-nodes_6-8                   0.056737          0.033057  ...   \n",
            "inv-nodes_9-11                  0.028786          0.062707  ...   \n",
            "node-caps_no                    0.009516          0.148652  ...   \n",
            "node-caps_yes                   0.046392          0.132905  ...   \n",
            "breast_right                    0.066228          0.026381  ...   \n",
            "breast-quad_central             0.054060          0.047631  ...   \n",
            "breast-quad_left_low            0.004428          0.029765  ...   \n",
            "breast-quad_left_up             0.057294          0.062217  ...   \n",
            "breast-quad_right_low           0.086174          0.014837  ...   \n",
            "breast-quad_right_up            0.059001          0.082151  ...   \n",
            "irradiat_yes                    0.054859          0.101079  ...   \n",
            "\n",
            "                       inv-nodes_9-11  node-caps_no  node-caps_yes  \\\n",
            "deg-malig                    0.090675      0.271713       0.325930   \n",
            "Class                        0.126112      0.274964       0.276792   \n",
            "age_30-39                    0.042536      0.001415       0.025263   \n",
            "age_40-49                    0.006019      0.020592       0.007165   \n",
            "age_50-59                    0.025933      0.062491       0.041103   \n",
            "age_60-69                    0.094965      0.036862       0.047662   \n",
            "age_70-79                    0.104954      0.020059       0.072232   \n",
            "menopause_lt40               0.030150      0.023540       0.078159   \n",
            "menopause_premeno            0.028786      0.009516       0.046392   \n",
            "tumor-size_10-14             0.062707      0.148652       0.132905   \n",
            "tumor-size_15-19             0.003041      0.046908       0.053892   \n",
            "tumor-size_20-24             0.087614      0.004170       0.018332   \n",
            "tumor-size_25-29             0.043195      0.019638       0.012911   \n",
            "tumor-size_30-34             0.088923      0.094235       0.092012   \n",
            "tumor-size_35-39             0.178521      0.092581       0.116041   \n",
            "tumor-size_40-44             0.054948      0.033906       0.055960   \n",
            "tumor-size_45-49             0.019598      0.027065       0.035684   \n",
            "tumor-size_5-9               0.022670      0.063947       0.058767   \n",
            "tumor-size_50-54             0.083147      0.010674       0.023168   \n",
            "inv-nodes_12-14              0.019598      0.109412       0.122171   \n",
            "inv-nodes_15-17              0.027864      0.214097       0.235182   \n",
            "inv-nodes_24-26              0.011275      0.110322       0.120046   \n",
            "inv-nodes_3-5                0.072232      0.327377       0.317463   \n",
            "inv-nodes_6-8                0.047851      0.361760       0.397673   \n",
            "inv-nodes_9-11               1.000000      0.308841       0.193886   \n",
            "node-caps_no                 0.308841      1.000000       0.919002   \n",
            "node-caps_yes                0.193886      0.919002       1.000000   \n",
            "breast_right                 0.012004      0.033389       0.004198   \n",
            "breast-quad_central          0.053584      0.054656       0.037561   \n",
            "breast-quad_left_low         0.033106      0.006632       0.009752   \n",
            "breast-quad_left_up          0.064668      0.012516       0.018482   \n",
            "breast-quad_right_low        0.011042      0.019044       0.009556   \n",
            "breast-quad_right_up         0.009166      0.042419       0.070010   \n",
            "irradiat_yes                 0.206678      0.370158       0.303955   \n",
            "\n",
            "                       breast_right  breast-quad_central  \\\n",
            "deg-malig                  0.052861             0.073271   \n",
            "Class                      0.058646             0.065738   \n",
            "age_30-39                  0.039441             0.095237   \n",
            "age_40-49                  0.103086             0.104159   \n",
            "age_50-59                  0.073879             0.026998   \n",
            "age_60-69                  0.005152             0.006219   \n",
            "age_70-79                  0.039661             0.052328   \n",
            "menopause_lt40             0.058030             0.044590   \n",
            "menopause_premeno          0.066228             0.054060   \n",
            "tumor-size_10-14           0.026381             0.047631   \n",
            "tumor-size_15-19           0.021588             0.034876   \n",
            "tumor-size_20-24           0.010579             0.011600   \n",
            "tumor-size_25-29           0.005384             0.067307   \n",
            "tumor-size_30-34           0.053553             0.013354   \n",
            "tumor-size_35-39           0.002755             0.075094   \n",
            "tumor-size_40-44           0.044500             0.081264   \n",
            "tumor-size_45-49           0.027895             0.102593   \n",
            "tumor-size_5-9             0.052157             0.080624   \n",
            "tumor-size_50-54           0.095685             0.047754   \n",
            "inv-nodes_12-14            0.027895             0.028984   \n",
            "inv-nodes_15-17            0.009231             0.041208   \n",
            "inv-nodes_24-26            0.055617             0.016675   \n",
            "inv-nodes_3-5              0.108425             0.025999   \n",
            "inv-nodes_6-8              0.087866             0.014074   \n",
            "inv-nodes_9-11             0.012004             0.053584   \n",
            "node-caps_no               0.033389             0.054656   \n",
            "node-caps_yes              0.004198             0.037561   \n",
            "breast_right               1.000000             0.004321   \n",
            "breast-quad_central        0.004321             1.000000   \n",
            "breast-quad_left_low       0.281404             0.222550   \n",
            "breast-quad_left_up        0.230183             0.201670   \n",
            "breast-quad_right_low      0.107272             0.085200   \n",
            "breast-quad_right_up       0.187264             0.101668   \n",
            "irradiat_yes               0.018761             0.094245   \n",
            "\n",
            "                       breast-quad_left_low  breast-quad_left_up  \\\n",
            "deg-malig                          0.013504             0.052634   \n",
            "Class                              0.036290             0.045711   \n",
            "age_30-39                          0.003333             0.049202   \n",
            "age_40-49                          0.025000             0.007563   \n",
            "age_50-59                          0.016390             0.008750   \n",
            "age_60-69                          0.037366             0.049323   \n",
            "age_70-79                          0.065579             0.001802   \n",
            "menopause_lt40                     0.014311             0.029915   \n",
            "menopause_premeno                  0.004428             0.057294   \n",
            "tumor-size_10-14                   0.029765             0.062217   \n",
            "tumor-size_15-19                   0.081190             0.052420   \n",
            "tumor-size_20-24                   0.004367             0.059151   \n",
            "tumor-size_25-29                   0.022602             0.024810   \n",
            "tumor-size_30-34                   0.089616             0.011797   \n",
            "tumor-size_35-39                   0.048838             0.013169   \n",
            "tumor-size_40-44                   0.014523             0.042641   \n",
            "tumor-size_45-49                   0.010853             0.073760   \n",
            "tumor-size_5-9                     0.032954             0.085322   \n",
            "tumor-size_50-54                   0.003353             0.057632   \n",
            "inv-nodes_12-14                    0.081397             0.001267   \n",
            "inv-nodes_15-17                    0.034718             0.001802   \n",
            "inv-nodes_24-26                    0.074927             0.042436   \n",
            "inv-nodes_3-5                      0.068333             0.004671   \n",
            "inv-nodes_6-8                      0.014029             0.055154   \n",
            "inv-nodes_9-11                     0.033106             0.064668   \n",
            "node-caps_no                       0.006632             0.012516   \n",
            "node-caps_yes                      0.009752             0.018482   \n",
            "breast_right                       0.281404             0.230183   \n",
            "breast-quad_central                0.222550             0.201670   \n",
            "breast-quad_left_low               1.000000             0.566363   \n",
            "breast-quad_left_up                0.566363             1.000000   \n",
            "breast-quad_right_low              0.239274             0.216825   \n",
            "breast-quad_right_up               0.285520             0.258733   \n",
            "irradiat_yes                       0.031167             0.033605   \n",
            "\n",
            "                       breast-quad_right_low  breast-quad_right_up  \\\n",
            "deg-malig                           0.037217              0.020563   \n",
            "Class                               0.031259              0.076444   \n",
            "age_30-39                           0.037220              0.038069   \n",
            "age_40-49                           0.093621              0.038069   \n",
            "age_50-59                           0.054905              0.001783   \n",
            "age_60-69                           0.087862              0.043202   \n",
            "age_70-79                           0.043690              0.023497   \n",
            "menopause_lt40                      0.047940              0.013621   \n",
            "menopause_premeno                   0.086174              0.059001   \n",
            "tumor-size_10-14                    0.014837              0.082151   \n",
            "tumor-size_15-19                    0.019858              0.087917   \n",
            "tumor-size_20-24                    0.039704              0.050979   \n",
            "tumor-size_25-29                    0.111767              0.034418   \n",
            "tumor-size_30-34                    0.001083              0.109585   \n",
            "tumor-size_35-39                    0.080738              0.079440   \n",
            "tumor-size_40-44                    0.087370              0.060027   \n",
            "tumor-size_45-49                    0.031162              0.070238   \n",
            "tumor-size_5-9                      0.071342              0.050182   \n",
            "tumor-size_50-54                    0.051343              0.005105   \n",
            "inv-nodes_12-14                     0.216401              0.037185   \n",
            "inv-nodes_15-17                     0.044305              0.023497   \n",
            "inv-nodes_24-26                     0.017928              0.021393   \n",
            "inv-nodes_3-5                       0.114851              0.027918   \n",
            "inv-nodes_6-8                       0.083918              0.001780   \n",
            "inv-nodes_9-11                      0.011042              0.009166   \n",
            "node-caps_no                        0.019044              0.042419   \n",
            "node-caps_yes                       0.009556              0.070010   \n",
            "breast_right                        0.107272              0.187264   \n",
            "breast-quad_central                 0.085200              0.101668   \n",
            "breast-quad_left_low                0.239274              0.285520   \n",
            "breast-quad_left_up                 0.216825              0.258733   \n",
            "breast-quad_right_low               1.000000              0.109308   \n",
            "breast-quad_right_up                0.109308              1.000000   \n",
            "irradiat_yes                        0.038323              0.047461   \n",
            "\n",
            "                       irradiat_yes  \n",
            "deg-malig                  0.208099  \n",
            "Class                      0.193912  \n",
            "age_30-39                  0.035669  \n",
            "age_40-49                  0.081381  \n",
            "age_50-59                  0.136102  \n",
            "age_60-69                  0.050322  \n",
            "age_70-79                  0.024447  \n",
            "menopause_lt40             0.088465  \n",
            "menopause_premeno          0.054859  \n",
            "tumor-size_10-14           0.101079  \n",
            "tumor-size_15-19           0.030366  \n",
            "tumor-size_20-24           0.062453  \n",
            "tumor-size_25-29           0.087322  \n",
            "tumor-size_30-34           0.034984  \n",
            "tumor-size_35-39           0.017067  \n",
            "tumor-size_40-44           0.054532  \n",
            "tumor-size_45-49           0.103732  \n",
            "tumor-size_5-9             0.003424  \n",
            "tumor-size_50-54           0.054686  \n",
            "inv-nodes_12-14            0.184349  \n",
            "inv-nodes_15-17            0.024447  \n",
            "inv-nodes_24-26            0.106060  \n",
            "inv-nodes_3-5              0.208994  \n",
            "inv-nodes_6-8              0.172224  \n",
            "inv-nodes_9-11             0.206678  \n",
            "node-caps_no               0.370158  \n",
            "node-caps_yes              0.303955  \n",
            "breast_right               0.018761  \n",
            "breast-quad_central        0.094245  \n",
            "breast-quad_left_low       0.031167  \n",
            "breast-quad_left_up        0.033605  \n",
            "breast-quad_right_low      0.038323  \n",
            "breast-quad_right_up       0.047461  \n",
            "irradiat_yes               1.000000  \n",
            "\n",
            "[34 rows x 34 columns]\n"
          ]
        }
      ],
      "source": [
        "correlation_matrix = data.corr().abs()\n",
        "upper_tri = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape, dtype=bool), k=1))\n",
        "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.7)]  # Ajuste o limite de correlação conforme necessário\n",
        "data = data.drop(columns=to_drop)\n",
        "\n",
        "print(correlation_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = data.drop('Class', axis=1)\n",
        "y = data['Class']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Balanceando os dados**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Antes do oversampling: Counter({0: 201, 1: 85})\n",
            "Depois do oversampling: Counter({0: 201, 1: 100})\n"
          ]
        }
      ],
      "source": [
        "smote = SMOTE(sampling_strategy=0.5, random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "print(f'Antes do oversampling: {Counter(y)}')\n",
        "print(f'Depois do oversampling: {Counter(y_resampled)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_treino, X_teste, y_treino, y_teste = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Identificar outliers e normalizar os dados**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "Q1 = X_treino.quantile(0.25)\n",
        "Q3 = X_treino.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "outliers = (X_treino < (Q1 - 1.5 * IQR)) | (X_treino > (Q3 + 1.5 * IQR))\n",
        "X_treino[outliers] = np.nan\n",
        "X_treino = X_treino.fillna(X_treino.median())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Padronizar os dados**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_treino = scaler.fit_transform(X_treino)\n",
        "X_teste = scaler.transform(X_teste)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bCxFBVNFt22"
      },
      "source": [
        "**Vamos treinar com a rede neural?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QRptikHHepQ",
        "outputId": "f0106c94-1301-44e9-be60-e108bf82bebb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.70732544\n",
            "Iteration 2, loss = 0.69383296\n",
            "Iteration 3, loss = 0.68265020\n",
            "Iteration 4, loss = 0.67124367\n",
            "Iteration 5, loss = 0.66137069\n",
            "Iteration 6, loss = 0.65197949\n",
            "Iteration 7, loss = 0.64331646\n",
            "Iteration 8, loss = 0.63573944\n",
            "Iteration 9, loss = 0.62889826\n",
            "Iteration 10, loss = 0.62287545\n",
            "Iteration 11, loss = 0.61745211\n",
            "Iteration 12, loss = 0.61286034\n",
            "Iteration 13, loss = 0.60827574\n",
            "Iteration 14, loss = 0.60456016\n",
            "Iteration 15, loss = 0.60191988\n",
            "Iteration 16, loss = 0.59841436\n",
            "Iteration 17, loss = 0.59601203\n",
            "Iteration 18, loss = 0.59395787\n",
            "Iteration 19, loss = 0.59167817\n",
            "Iteration 20, loss = 0.58997840\n",
            "Iteration 21, loss = 0.58849963\n",
            "Iteration 22, loss = 0.58713538\n",
            "Iteration 23, loss = 0.58621375\n",
            "Iteration 24, loss = 0.58481255\n",
            "Iteration 25, loss = 0.58401477\n",
            "Iteration 26, loss = 0.58313574\n",
            "Iteration 27, loss = 0.58246531\n",
            "Iteration 28, loss = 0.58164588\n",
            "Iteration 29, loss = 0.58102804\n",
            "Iteration 30, loss = 0.58043884\n",
            "Iteration 31, loss = 0.57978109\n",
            "Iteration 32, loss = 0.57920871\n",
            "Iteration 33, loss = 0.57859269\n",
            "Iteration 34, loss = 0.57799884\n",
            "Iteration 35, loss = 0.57739450\n",
            "Iteration 36, loss = 0.57676276\n",
            "Iteration 37, loss = 0.57624215\n",
            "Iteration 38, loss = 0.57582484\n",
            "Iteration 39, loss = 0.57532576\n",
            "Iteration 40, loss = 0.57493138\n",
            "Iteration 41, loss = 0.57441848\n",
            "Iteration 42, loss = 0.57398726\n",
            "Iteration 43, loss = 0.57351384\n",
            "Iteration 44, loss = 0.57303890\n",
            "Iteration 45, loss = 0.57259754\n",
            "Iteration 46, loss = 0.57207599\n",
            "Iteration 47, loss = 0.57163916\n",
            "Iteration 48, loss = 0.57121958\n",
            "Iteration 49, loss = 0.57078645\n",
            "Iteration 50, loss = 0.57024738\n",
            "Iteration 51, loss = 0.56985452\n",
            "Iteration 52, loss = 0.56941831\n",
            "Iteration 53, loss = 0.56908861\n",
            "Iteration 54, loss = 0.56873891\n",
            "Iteration 55, loss = 0.56839856\n",
            "Iteration 56, loss = 0.56784349\n",
            "Iteration 57, loss = 0.56742208\n",
            "Iteration 58, loss = 0.56694542\n",
            "Iteration 59, loss = 0.56645648\n",
            "Iteration 60, loss = 0.56596695\n",
            "Iteration 61, loss = 0.56557913\n",
            "Iteration 62, loss = 0.56511490\n",
            "Iteration 63, loss = 0.56470718\n",
            "Iteration 64, loss = 0.56442333\n",
            "Iteration 65, loss = 0.56394610\n",
            "Iteration 66, loss = 0.56349648\n",
            "Iteration 67, loss = 0.56318944\n",
            "Iteration 68, loss = 0.56295742\n",
            "Iteration 69, loss = 0.56260896\n",
            "Iteration 70, loss = 0.56223533\n",
            "Iteration 71, loss = 0.56179986\n",
            "Iteration 72, loss = 0.56134061\n",
            "Iteration 73, loss = 0.56095559\n",
            "Iteration 74, loss = 0.56060227\n",
            "Iteration 75, loss = 0.56028223\n",
            "Iteration 76, loss = 0.55993215\n",
            "Iteration 77, loss = 0.55968449\n",
            "Iteration 78, loss = 0.55930323\n",
            "Iteration 79, loss = 0.55899897\n",
            "Iteration 80, loss = 0.55867980\n",
            "Iteration 81, loss = 0.55831707\n",
            "Iteration 82, loss = 0.55795840\n",
            "Iteration 83, loss = 0.55764044\n",
            "Iteration 84, loss = 0.55710987\n",
            "Iteration 85, loss = 0.55685388\n",
            "Iteration 86, loss = 0.55653995\n",
            "Iteration 87, loss = 0.55619036\n",
            "Iteration 88, loss = 0.55573480\n",
            "Iteration 89, loss = 0.55547096\n",
            "Iteration 90, loss = 0.55503507\n",
            "Iteration 91, loss = 0.55470783\n",
            "Iteration 92, loss = 0.55438734\n",
            "Iteration 93, loss = 0.55401855\n",
            "Iteration 94, loss = 0.55379345\n",
            "Iteration 95, loss = 0.55360254\n",
            "Iteration 96, loss = 0.55327192\n",
            "Iteration 97, loss = 0.55294284\n",
            "Iteration 98, loss = 0.55262734\n",
            "Iteration 99, loss = 0.55221992\n",
            "Iteration 100, loss = 0.55179543\n",
            "Iteration 101, loss = 0.55152096\n",
            "Iteration 102, loss = 0.55104514\n",
            "Iteration 103, loss = 0.55075116\n",
            "Iteration 104, loss = 0.55048441\n",
            "Iteration 105, loss = 0.55030896\n",
            "Iteration 106, loss = 0.55002790\n",
            "Iteration 107, loss = 0.54986675\n",
            "Iteration 108, loss = 0.54952813\n",
            "Iteration 109, loss = 0.54923207\n",
            "Iteration 110, loss = 0.54892655\n",
            "Iteration 111, loss = 0.54860114\n",
            "Iteration 112, loss = 0.54833387\n",
            "Iteration 113, loss = 0.54804839\n",
            "Iteration 114, loss = 0.54769000\n",
            "Iteration 115, loss = 0.54736056\n",
            "Iteration 116, loss = 0.54715843\n",
            "Iteration 117, loss = 0.54681802\n",
            "Iteration 118, loss = 0.54662437\n",
            "Iteration 119, loss = 0.54638280\n",
            "Iteration 120, loss = 0.54615257\n",
            "Iteration 121, loss = 0.54593053\n",
            "Iteration 122, loss = 0.54572970\n",
            "Iteration 123, loss = 0.54532598\n",
            "Iteration 124, loss = 0.54493142\n",
            "Iteration 125, loss = 0.54462788\n",
            "Iteration 126, loss = 0.54440186\n",
            "Iteration 127, loss = 0.54403399\n",
            "Iteration 128, loss = 0.54376858\n",
            "Iteration 129, loss = 0.54360244\n",
            "Iteration 130, loss = 0.54314441\n",
            "Iteration 131, loss = 0.54292405\n",
            "Iteration 132, loss = 0.54264459\n",
            "Iteration 133, loss = 0.54226055\n",
            "Iteration 134, loss = 0.54186211\n",
            "Iteration 135, loss = 0.54155192\n",
            "Iteration 136, loss = 0.54134776\n",
            "Iteration 137, loss = 0.54114411\n",
            "Iteration 138, loss = 0.54081959\n",
            "Iteration 139, loss = 0.54053002\n",
            "Iteration 140, loss = 0.54044667\n",
            "Iteration 141, loss = 0.54013368\n",
            "Iteration 142, loss = 0.54003986\n",
            "Iteration 143, loss = 0.53986225\n",
            "Iteration 144, loss = 0.53960706\n",
            "Iteration 145, loss = 0.53956643\n",
            "Iteration 146, loss = 0.53932676\n",
            "Iteration 147, loss = 0.53901092\n",
            "Iteration 148, loss = 0.53857494\n",
            "Iteration 149, loss = 0.53814191\n",
            "Iteration 150, loss = 0.53771305\n",
            "Iteration 151, loss = 0.53716918\n",
            "Iteration 152, loss = 0.53673981\n",
            "Iteration 153, loss = 0.53627851\n",
            "Iteration 154, loss = 0.53601053\n",
            "Iteration 155, loss = 0.53571781\n",
            "Iteration 156, loss = 0.53544711\n",
            "Iteration 157, loss = 0.53521325\n",
            "Iteration 158, loss = 0.53499327\n",
            "Iteration 159, loss = 0.53461242\n",
            "Iteration 160, loss = 0.53438573\n",
            "Iteration 161, loss = 0.53415329\n",
            "Iteration 162, loss = 0.53376995\n",
            "Iteration 163, loss = 0.53348107\n",
            "Iteration 164, loss = 0.53323532\n",
            "Iteration 165, loss = 0.53300760\n",
            "Iteration 166, loss = 0.53280486\n",
            "Iteration 167, loss = 0.53270041\n",
            "Iteration 168, loss = 0.53229197\n",
            "Iteration 169, loss = 0.53199469\n",
            "Iteration 170, loss = 0.53176975\n",
            "Iteration 171, loss = 0.53147795\n",
            "Iteration 172, loss = 0.53153192\n",
            "Iteration 173, loss = 0.53119397\n",
            "Iteration 174, loss = 0.53111251\n",
            "Iteration 175, loss = 0.53067707\n",
            "Iteration 176, loss = 0.53053079\n",
            "Iteration 177, loss = 0.53031418\n",
            "Iteration 178, loss = 0.53011537\n",
            "Iteration 179, loss = 0.52988309\n",
            "Iteration 180, loss = 0.52959886\n",
            "Iteration 181, loss = 0.52936446\n",
            "Iteration 182, loss = 0.52911057\n",
            "Iteration 183, loss = 0.52872736\n",
            "Iteration 184, loss = 0.52884127\n",
            "Iteration 185, loss = 0.52881085\n",
            "Iteration 186, loss = 0.52860194\n",
            "Iteration 187, loss = 0.52844700\n",
            "Iteration 188, loss = 0.52824688\n",
            "Iteration 189, loss = 0.52790802\n",
            "Iteration 190, loss = 0.52758415\n",
            "Iteration 191, loss = 0.52723849\n",
            "Iteration 192, loss = 0.52715980\n",
            "Iteration 193, loss = 0.52689211\n",
            "Iteration 194, loss = 0.52674592\n",
            "Iteration 195, loss = 0.52646318\n",
            "Iteration 196, loss = 0.52621957\n",
            "Iteration 197, loss = 0.52599947\n",
            "Iteration 198, loss = 0.52572343\n",
            "Iteration 199, loss = 0.52539848\n",
            "Iteration 200, loss = 0.52517400\n",
            "Iteration 201, loss = 0.52484070\n",
            "Iteration 202, loss = 0.52459275\n",
            "Iteration 203, loss = 0.52454883\n",
            "Iteration 204, loss = 0.52419470\n",
            "Iteration 205, loss = 0.52379852\n",
            "Iteration 206, loss = 0.52352351\n",
            "Iteration 207, loss = 0.52320684\n",
            "Iteration 208, loss = 0.52299623\n",
            "Iteration 209, loss = 0.52273313\n",
            "Iteration 210, loss = 0.52264086\n",
            "Iteration 211, loss = 0.52234031\n",
            "Iteration 212, loss = 0.52206488\n",
            "Iteration 213, loss = 0.52187497\n",
            "Iteration 214, loss = 0.52170662\n",
            "Iteration 215, loss = 0.52139053\n",
            "Iteration 216, loss = 0.52114050\n",
            "Iteration 217, loss = 0.52106814\n",
            "Iteration 218, loss = 0.52070406\n",
            "Iteration 219, loss = 0.52040995\n",
            "Iteration 220, loss = 0.52009948\n",
            "Iteration 221, loss = 0.51996325\n",
            "Iteration 222, loss = 0.51965778\n",
            "Iteration 223, loss = 0.51942445\n",
            "Iteration 224, loss = 0.51919991\n",
            "Iteration 225, loss = 0.51911524\n",
            "Iteration 226, loss = 0.51878552\n",
            "Iteration 227, loss = 0.51854514\n",
            "Iteration 228, loss = 0.51848478\n",
            "Iteration 229, loss = 0.51809330\n",
            "Iteration 230, loss = 0.51767416\n",
            "Iteration 231, loss = 0.51749465\n",
            "Iteration 232, loss = 0.51704482\n",
            "Iteration 233, loss = 0.51687618\n",
            "Iteration 234, loss = 0.51653642\n",
            "Iteration 235, loss = 0.51650202\n",
            "Iteration 236, loss = 0.51618479\n",
            "Iteration 237, loss = 0.51630079\n",
            "Iteration 238, loss = 0.51623500\n",
            "Iteration 239, loss = 0.51608653\n",
            "Iteration 240, loss = 0.51592924\n",
            "Iteration 241, loss = 0.51569799\n",
            "Iteration 242, loss = 0.51553125\n",
            "Iteration 243, loss = 0.51527690\n",
            "Iteration 244, loss = 0.51490840\n",
            "Iteration 245, loss = 0.51459232\n",
            "Iteration 246, loss = 0.51435391\n",
            "Iteration 247, loss = 0.51417920\n",
            "Iteration 248, loss = 0.51395593\n",
            "Iteration 249, loss = 0.51373710\n",
            "Iteration 250, loss = 0.51371276\n",
            "Iteration 251, loss = 0.51342525\n",
            "Iteration 252, loss = 0.51319776\n",
            "Iteration 253, loss = 0.51313838\n",
            "Iteration 254, loss = 0.51274703\n",
            "Iteration 255, loss = 0.51268701\n",
            "Iteration 256, loss = 0.51237198\n",
            "Iteration 257, loss = 0.51217543\n",
            "Iteration 258, loss = 0.51206261\n",
            "Iteration 259, loss = 0.51179685\n",
            "Iteration 260, loss = 0.51173793\n",
            "Iteration 261, loss = 0.51169778\n",
            "Iteration 262, loss = 0.51138309\n",
            "Iteration 263, loss = 0.51100659\n",
            "Iteration 264, loss = 0.51083679\n",
            "Iteration 265, loss = 0.51061527\n",
            "Iteration 266, loss = 0.51041317\n",
            "Iteration 267, loss = 0.51020091\n",
            "Iteration 268, loss = 0.50998596\n",
            "Iteration 269, loss = 0.50972754\n",
            "Iteration 270, loss = 0.50974683\n",
            "Iteration 271, loss = 0.50938323\n",
            "Iteration 272, loss = 0.50912456\n",
            "Iteration 273, loss = 0.50889073\n",
            "Iteration 274, loss = 0.50874324\n",
            "Iteration 275, loss = 0.50855320\n",
            "Iteration 276, loss = 0.50880588\n",
            "Iteration 277, loss = 0.50855698\n",
            "Iteration 278, loss = 0.50866950\n",
            "Iteration 279, loss = 0.50873657\n",
            "Iteration 280, loss = 0.50875182\n",
            "Iteration 281, loss = 0.50851166\n",
            "Iteration 282, loss = 0.50833995\n",
            "Iteration 283, loss = 0.50804371\n",
            "Iteration 284, loss = 0.50783628\n",
            "Iteration 285, loss = 0.50783746\n",
            "Iteration 286, loss = 0.50754130\n",
            "Iteration 287, loss = 0.50726630\n",
            "Iteration 288, loss = 0.50703386\n",
            "Iteration 289, loss = 0.50682252\n",
            "Iteration 290, loss = 0.50654089\n",
            "Iteration 291, loss = 0.50680172\n",
            "Iteration 292, loss = 0.50672831\n",
            "Iteration 293, loss = 0.50655330\n",
            "Iteration 294, loss = 0.50640383\n",
            "Iteration 295, loss = 0.50591719\n",
            "Iteration 296, loss = 0.50569555\n",
            "Iteration 297, loss = 0.50525278\n",
            "Iteration 298, loss = 0.50497734\n",
            "Iteration 299, loss = 0.50494108\n",
            "Iteration 300, loss = 0.50463866\n",
            "Iteration 301, loss = 0.50444651\n",
            "Iteration 302, loss = 0.50417670\n",
            "Iteration 303, loss = 0.50390849\n",
            "Iteration 304, loss = 0.50361023\n",
            "Iteration 305, loss = 0.50337373\n",
            "Iteration 306, loss = 0.50329709\n",
            "Iteration 307, loss = 0.50304371\n",
            "Iteration 308, loss = 0.50286370\n",
            "Iteration 309, loss = 0.50260988\n",
            "Iteration 310, loss = 0.50236279\n",
            "Iteration 311, loss = 0.50235946\n",
            "Iteration 312, loss = 0.50227196\n",
            "Iteration 313, loss = 0.50221229\n",
            "Iteration 314, loss = 0.50240618\n",
            "Iteration 315, loss = 0.50239419\n",
            "Iteration 316, loss = 0.50242356\n",
            "Iteration 317, loss = 0.50245231\n",
            "Iteration 318, loss = 0.50242091\n",
            "Iteration 319, loss = 0.50231701\n",
            "Iteration 320, loss = 0.50212691\n",
            "Iteration 321, loss = 0.50188129\n",
            "Iteration 322, loss = 0.50155706\n",
            "Iteration 323, loss = 0.50115622\n",
            "Iteration 324, loss = 0.50090312\n",
            "Iteration 325, loss = 0.50053049\n",
            "Iteration 326, loss = 0.50015337\n",
            "Iteration 327, loss = 0.49979407\n",
            "Iteration 328, loss = 0.49977116\n",
            "Iteration 329, loss = 0.49926918\n",
            "Iteration 330, loss = 0.49928799\n",
            "Iteration 331, loss = 0.49893348\n",
            "Iteration 332, loss = 0.49868112\n",
            "Iteration 333, loss = 0.49880638\n",
            "Iteration 334, loss = 0.49913039\n",
            "Iteration 335, loss = 0.49856670\n",
            "Iteration 336, loss = 0.49839529\n",
            "Iteration 337, loss = 0.49811436\n",
            "Iteration 338, loss = 0.49810789\n",
            "Iteration 339, loss = 0.49794347\n",
            "Iteration 340, loss = 0.49769337\n",
            "Iteration 341, loss = 0.49740493\n",
            "Iteration 342, loss = 0.49752802\n",
            "Iteration 343, loss = 0.49777690\n",
            "Iteration 344, loss = 0.49802537\n",
            "Iteration 345, loss = 0.49781311\n",
            "Iteration 346, loss = 0.49760237\n",
            "Iteration 347, loss = 0.49736706\n",
            "Iteration 348, loss = 0.49714551\n",
            "Iteration 349, loss = 0.49682395\n",
            "Iteration 350, loss = 0.49642953\n",
            "Iteration 351, loss = 0.49629850\n",
            "Iteration 352, loss = 0.49593114\n",
            "Iteration 353, loss = 0.49566329\n",
            "Iteration 354, loss = 0.49541893\n",
            "Iteration 355, loss = 0.49530285\n",
            "Iteration 356, loss = 0.49528695\n",
            "Iteration 357, loss = 0.49510807\n",
            "Iteration 358, loss = 0.49508972\n",
            "Iteration 359, loss = 0.49503889\n",
            "Iteration 360, loss = 0.49499752\n",
            "Iteration 361, loss = 0.49472501\n",
            "Iteration 362, loss = 0.49448674\n",
            "Iteration 363, loss = 0.49409519\n",
            "Iteration 364, loss = 0.49400040\n",
            "Iteration 365, loss = 0.49424059\n",
            "Iteration 366, loss = 0.49427062\n",
            "Iteration 367, loss = 0.49438063\n",
            "Iteration 368, loss = 0.49428480\n",
            "Iteration 369, loss = 0.49421309\n",
            "Iteration 370, loss = 0.49427661\n",
            "Iteration 371, loss = 0.49416120\n",
            "Iteration 372, loss = 0.49378644\n",
            "Iteration 373, loss = 0.49342208\n",
            "Iteration 374, loss = 0.49302974\n",
            "Iteration 375, loss = 0.49281315\n",
            "Iteration 376, loss = 0.49254388\n",
            "Iteration 377, loss = 0.49235410\n",
            "Iteration 378, loss = 0.49217383\n",
            "Iteration 379, loss = 0.49197918\n",
            "Iteration 380, loss = 0.49188209\n",
            "Iteration 381, loss = 0.49162508\n",
            "Iteration 382, loss = 0.49143638\n",
            "Iteration 383, loss = 0.49130398\n",
            "Iteration 384, loss = 0.49111318\n",
            "Iteration 385, loss = 0.49124013\n",
            "Iteration 386, loss = 0.49119566\n",
            "Iteration 387, loss = 0.49103246\n",
            "Iteration 388, loss = 0.49108305\n",
            "Iteration 389, loss = 0.49092459\n",
            "Iteration 390, loss = 0.49085153\n",
            "Iteration 391, loss = 0.49075578\n",
            "Iteration 392, loss = 0.49082928\n",
            "Iteration 393, loss = 0.49083371\n",
            "Iteration 394, loss = 0.49083982\n",
            "Iteration 395, loss = 0.49068916\n",
            "Iteration 396, loss = 0.49051948\n",
            "Iteration 397, loss = 0.49023320\n",
            "Iteration 398, loss = 0.49008658\n",
            "Iteration 399, loss = 0.48988160\n",
            "Iteration 400, loss = 0.48977993\n",
            "Iteration 401, loss = 0.48963945\n",
            "Iteration 402, loss = 0.48955918\n",
            "Iteration 403, loss = 0.48947195\n",
            "Iteration 404, loss = 0.48936601\n",
            "Iteration 405, loss = 0.48923545\n",
            "Iteration 406, loss = 0.48925088\n",
            "Iteration 407, loss = 0.48906631\n",
            "Iteration 408, loss = 0.48916150\n",
            "Iteration 409, loss = 0.48899419\n",
            "Iteration 410, loss = 0.48893537\n",
            "Iteration 411, loss = 0.48850308\n",
            "Iteration 412, loss = 0.48829015\n",
            "Iteration 413, loss = 0.48791651\n",
            "Iteration 414, loss = 0.48740656\n",
            "Iteration 415, loss = 0.48769099\n",
            "Iteration 416, loss = 0.48719348\n",
            "Iteration 417, loss = 0.48739406\n",
            "Iteration 418, loss = 0.48733006\n",
            "Iteration 419, loss = 0.48750153\n",
            "Iteration 420, loss = 0.48739559\n",
            "Iteration 421, loss = 0.48731201\n",
            "Iteration 422, loss = 0.48744726\n",
            "Iteration 423, loss = 0.48695128\n",
            "Iteration 424, loss = 0.48685292\n",
            "Iteration 425, loss = 0.48706560\n",
            "Iteration 426, loss = 0.48692999\n",
            "Iteration 427, loss = 0.48713726\n",
            "Iteration 428, loss = 0.48731520\n",
            "Iteration 429, loss = 0.48723551\n",
            "Iteration 430, loss = 0.48702474\n",
            "Iteration 431, loss = 0.48707452\n",
            "Iteration 432, loss = 0.48719009\n",
            "Iteration 433, loss = 0.48729938\n",
            "Iteration 434, loss = 0.48709790\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.7868852459016393"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "modelo = MLPClassifier(max_iter=1000, verbose=True)\n",
        "modelo.fit(X_treino, y_treino)\n",
        "\n",
        "previsoes = modelo.predict(X_teste)\n",
        "accuracy_score(y_teste,previsoes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Ajuste dos hiperparâmetros automaticamente**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Melhores Hiperparâmetros:\n",
            "{'activation': 'tanh', 'hidden_layer_sizes': (10, 10), 'max_iter': 1500, 'solver': 'sgd', 'tol': 0.0001, 'verbose': False}\n"
          ]
        }
      ],
      "source": [
        "param_grid = {\n",
        "    'max_iter': [500, 1000, 1500],  # Número máximo de iterações\n",
        "    'verbose': [True, False],  # Exibir informações de depuração\n",
        "    'tol': [1e-4, 1e-3, 1e-2],  # Tolerância para parar o treinamento\n",
        "    'solver': ['lbfgs', 'sgd', 'adam'],  # Otimizador\n",
        "    'activation': ['logistic', 'tanh', 'relu'],  # Função de ativação\n",
        "    'hidden_layer_sizes': [(5,), (10,), (5, 5), (10, 10), (10, 5, 2)],  # Topologias da rede\n",
        "}\n",
        "\n",
        "modelo = MLPClassifier()\n",
        "scoring = make_scorer(accuracy_score)\n",
        "\n",
        "grid_search = GridSearchCV(modelo, param_grid, cv=5, scoring=scoring, n_jobs=-1)\n",
        "grid_search.fit(X_treino, y_treino)\n",
        "\n",
        "print(\"Melhores Hiperparâmetros:\")\n",
        "print(grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PQZVMhEJXOC"
      },
      "source": [
        "**Modelo com os melhores parâmetros identificados pelo GSCV**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6Q1RssrJU9z",
        "outputId": "802b6904-3a64-4a71-fd3b-221c672f3524"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acurácia do Melhor Modelo: 0.6065573770491803\n"
          ]
        }
      ],
      "source": [
        "melhor_modelo = grid_search.best_estimator_\n",
        "previsoes = melhor_modelo.predict(X_teste)\n",
        "acuracia = accuracy_score(y_teste, previsoes)\n",
        "\n",
        "print(f'Acurácia do Melhor Modelo: {acuracia}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq-S4o3IczVP"
      },
      "source": [
        "\n",
        "\n",
        "> **Vamos testar o modelo?**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "1q9nsbSjdu23"
      },
      "outputs": [],
      "source": [
        "previsoes = melhor_modelo.predict(X_teste)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0PlSJE8fAUL",
        "outputId": "501c2371-84dd-4c64-f216-be2b03707888"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "previsoes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjWziqc5fV8m"
      },
      "source": [
        "\n",
        "\n",
        "> **Será se o modelo acertou?**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q92H3KOtfN5E",
        "outputId": "864bf803-aaec-436b-ee87-2bd4956fc72e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "177    1\n",
              "289    1\n",
              "228    0\n",
              "198    0\n",
              "60     1\n",
              "      ..\n",
              "234    0\n",
              "296    1\n",
              "281    0\n",
              "285    0\n",
              "182    0\n",
              "Name: Class, Length: 61, dtype: int64"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJ9MxYOIfmwv",
        "outputId": "d81573e1-45aa-4ea6-d922-c25e1a623343"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6065573770491803"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_score(y_teste,previsoes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3D5bvushr9W",
        "outputId": "45328eb3-d2be-4d4c-c5e1-248e262d7d9e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[37,  3],\n",
              "       [21,  0]], dtype=int64)"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "confusion_matrix(y_teste, previsoes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "wX15YT-7j-c9",
        "outputId": "db8b53f9-e72b-4257-c955-a229ab056d41"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6065573770491803"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x550 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "cm = ConfusionMatrix(melhor_modelo)\n",
        "cm.fit(X_treino, y_treino)\n",
        "cm.score(X_teste, y_teste)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIixOPw1kw-z",
        "outputId": "5b4735c0-0e3f-4981-b422-f4a5d25f5c76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.93      0.76        40\n",
            "           1       0.00      0.00      0.00        21\n",
            "\n",
            "    accuracy                           0.61        61\n",
            "   macro avg       0.32      0.46      0.38        61\n",
            "weighted avg       0.42      0.61      0.50        61\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(classification_report(y_teste, previsoes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csRtFY7lKr0N"
      },
      "source": [
        "**Veja como implementar o backpropagation em python:**\n",
        "https://www.askpython.com/python/examples/backpropagation-in-python\n",
        "https://www.deeplearningbook.com.br/algoritmo-backpropagation-em-python/\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
